{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eflores5000/08MIAR_APR/blob/main/proyecto_final_APR_V15_en_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVXkh8rCNl45"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D89Ue2uZNtzn"
      },
      "source": [
        "## 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7EQ46sNGLj22"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6on2PNxZN51A"
      },
      "source": [
        "## 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usPbIox-bl7J",
        "outputId": "926c16e9-8746-449b-dbd1-4c87132fd60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FFCbp3bbgj6",
        "outputId": "a4e13513-a7d6-4d3a-d837-c4438ae4392d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Archivos en el directorio: \n",
            "['dqn_models', 'checkpoints', 'dqn_05_weights.h5f.data-00000-of-00001', 'dqn_05_weights.h5f.index', 'dqn_06_weights.h5f.index', 'dqn_06_weights.h5f.data-00000-of-00001', 'dqn_07_weights.h5f.index', 'dqn_07_weights.h5f.data-00000-of-00001', 'dqn_08_weights.h5f.data-00000-of-00001', 'dqn_08_weights.h5f.index', '.ipynb_checkpoints', 'dqn_09_weights.h5f.index', 'dqn_09_weights.h5f.data-00000-of-00001', 'dqn_06_1_weights.h5f.data-00000-of-00001', 'dqn_06_1_weights.h5f.index', 'video', 'dqn_06_3_weights.h5f.index', 'dqn_06_3_weights.h5f.data-00000-of-00001', 'dqn_06_4_weights.h5f.index', 'dqn_06_4_weights.h5f.data-00000-of-00001', 'dqn_v0_weights.h5f.index', 'dqn_v0_weights.h5f.data-00000-of-00001', 'dqn_v06_4_weights.h5f.data-00000-of-00001', 'dqn_v06_4_weights.h5f.index', 'dqn_v06_5_weights.h5f.data-00000-of-00001', 'dqn_v06_5_weights.h5f.index', 'dqn_v06_5_1_weights.h5f.data-00000-of-00001', 'dqn_v06_5_1_weights.h5f.index', 'dqn_v06_x_weights.h5f.index', 'dqn_v06_x_weights.h5f.data-00000-of-00001', 'dqn_v06_x2_weights.h5f.data-00000-of-00001', 'dqn_v06_x2_weights.h5f.index', 'dqn_v11_weights.h5f.data-00000-of-00001', 'dqn_v11_weights.h5f.index', 'dqn_v12_1_weights.h5f.data-00000-of-00001', 'dqn_v12_1_weights.h5f.index', 'dqn_v15_weights.h5f.data-00000-of-00001', 'dqn_v15_weights.h5f.index', 'dqn_v14_weights.h5f.data-00000-of-00001', 'dqn_v14_weights.h5f.index', 'checkpoint']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYrJ03QWN9Hw"
      },
      "source": [
        "## 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSbNMsl4MTo3",
        "outputId": "f77c97a4-9cd1-455e-a00f-6e847cf76766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.23.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-vuzc9kv8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-vuzc9kv8\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.3.1)\n",
            "Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.1)\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.12  #2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.05\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.1.0\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9mL8Nw17Y5L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS60Dk-KO5e3"
      },
      "source": [
        "## **PARTE 3**. Desarrollo y preguntas\n",
        "Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RX-2z9T1NPp_"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Permute\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqJbsVR7POZA"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FlafEtvOKR4",
        "outputId": "10073a60-bc65-4a48-c6dd-1cda97758798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "210 160 3\n",
            "Forma de la observación: (210, 160, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "height, width, channels = env.observation_space.shape\n",
        "obs = env.reset()\n",
        "print(height, width, channels)\n",
        "print(\"Forma de la observación:\", obs.shape)  # Debe ser (height, width, channels)\n",
        "env.unwrapped.get_action_meanings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YSGPGszsOOAw"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        #return processed_observation.astype('uint8')\n",
        "        # Añadir canal para que sea (84, 84, 1)\n",
        "        return np.expand_dims(processed_observation, axis=-1).astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg54U9vFZEBQ"
      },
      "source": [
        "## 1. EJFM Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JhwO3z2oPVq3"
      },
      "outputs": [],
      "source": [
        "def build_model(height, width, channels, actions):\n",
        "\n",
        "    model = Sequential()\n",
        "    #model.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH, height, width)))\n",
        "    model.add(Conv2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(4,84,84,1))) # (4,height, width, channels)))\n",
        "    model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eBwiHKuZ6J_",
        "outputId": "180fa973-bb7d-4040-fc7a-3f0000e52fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 4, 20, 20, 32)     2080      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 4, 9, 9, 64)       32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 7, 7, 64)       36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               6423040   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,627,750\n",
            "Trainable params: 6,627,750\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#model = build_model(84, 84, 1, nb_actions)\n",
        "model = build_model(height, width, channels, nb_actions)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJoe_1sZOX4"
      },
      "source": [
        "## 2. EJFM Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_EDlLcIIPUsf"
      },
      "outputs": [],
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7FbbInVqR0sg"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(),\n",
        "        attr='eps',\n",
        "        value_max=1.0,\n",
        "        value_min=0.1,\n",
        "        value_test=0.2,\n",
        "        nb_steps=5000)                   # Pasos de exploración\n",
        "\n",
        "    memory = SequentialMemory(\n",
        "        limit=5000,                       # Replay Buffer (Memoria de experiencias pasadas en total (s,a,r,s'))\n",
        "        window_length=4)\n",
        "\n",
        "    dqn = DQNAgent(model=model,\n",
        "                   nb_actions=nb_actions,\n",
        "                   policy=policy,\n",
        "                   memory=memory,\n",
        "                   processor=AtariProcessor(),\n",
        "                   nb_steps_warmup=1000,    # Llena en buffer de pasos previos (s,a,r,s') sin evaluar aún.\n",
        "                   gamma=0.99,\n",
        "                   target_model_update=1000, # cada 10,000 steps actualiza la red objetivo\n",
        "                   train_interval=4,          # Solo entrena cada n pasos de interacción con el entorno\n",
        "                   delta_clip=1.0,            # Valor utilizado para recortar los errores TD (diferencia temporal) durante el entrenamiento para evitar fluctuaciones externas\n",
        "                   enable_double_dqn=True,\n",
        "                   enable_dueling_network=True,\n",
        "                   dueling_type='avg',\n",
        "                   batch_size=32,\n",
        "                   )\n",
        "    return dqn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-TmOagBXbWr4"
      },
      "outputs": [],
      "source": [
        "from rl.callbacks import ModelIntervalCheckpoint\n",
        "\n",
        "# Guardar el modelo cada 1000 pasos\n",
        "checkpoint_callback = ModelIntervalCheckpoint(\n",
        "    filepath='checkpoints/dqn_v15_weights_{step}.h5f',\n",
        "    interval=1000,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_a6zxl5ZbP4R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V-jxyOrdR0cg"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "dqn = build_agent(model, nb_actions)\n",
        "dqn.compile(Adam(learning_rate=0.0001, epsilon=0.001))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR2u9_7LR0WI",
        "outputId": "3b68bc4e-a5d0-44d9-d0a5-9dac005ae8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏱ Tiempo de inicio: 14:02:30\n",
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   721/100000: episode: 1, duration: 6.084s, episode steps: 721, steps per second: 119, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "Step 1000: saving model to checkpoints/dqn_v15_weights_1000.h5f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1236/100000: episode: 2, duration: 24.668s, episode steps: 515, steps per second:  21, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.005595, mean_q: 0.017218, mean_eps: 0.798760\n",
            "  1872/100000: episode: 3, duration: 57.501s, episode steps: 636, steps per second:  11, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.004870, mean_q: 0.019405, mean_eps: 0.720640\n",
            "Step 2000: saving model to checkpoints/dqn_v15_weights_2000.h5f\n",
            "  2735/100000: episode: 4, duration: 77.297s, episode steps: 863, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.015 [0.000, 5.000],  loss: 0.006710, mean_q: 0.023844, mean_eps: 0.585640\n",
            "Step 3000: saving model to checkpoints/dqn_v15_weights_3000.h5f\n",
            "  3817/100000: episode: 5, duration: 97.216s, episode steps: 1082, steps per second:  11, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.007315, mean_q: 0.038144, mean_eps: 0.410320\n",
            "Step 4000: saving model to checkpoints/dqn_v15_weights_4000.h5f\n",
            "  4524/100000: episode: 6, duration: 63.929s, episode steps: 707, steps per second:  11, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.008349, mean_q: 0.053442, mean_eps: 0.249400\n",
            "Step 5000: saving model to checkpoints/dqn_v15_weights_5000.h5f\n",
            "  5750/100000: episode: 7, duration: 111.562s, episode steps: 1226, steps per second:  11, episode reward: 17.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.006921, mean_q: 0.063858, mean_eps: 0.116745\n",
            "Step 6000: saving model to checkpoints/dqn_v15_weights_6000.h5f\n",
            "  6946/100000: episode: 8, duration: 110.090s, episode steps: 1196, steps per second:  11, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.008512, mean_q: 0.085121, mean_eps: 0.100000\n",
            "Step 7000: saving model to checkpoints/dqn_v15_weights_7000.h5f\n",
            "  7917/100000: episode: 9, duration: 87.906s, episode steps: 971, steps per second:  11, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.648 [0.000, 5.000],  loss: 0.007732, mean_q: 0.109899, mean_eps: 0.100000\n",
            "Step 8000: saving model to checkpoints/dqn_v15_weights_8000.h5f\n",
            "  8543/100000: episode: 10, duration: 57.839s, episode steps: 626, steps per second:  11, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.008621, mean_q: 0.119118, mean_eps: 0.100000\n",
            "Step 9000: saving model to checkpoints/dqn_v15_weights_9000.h5f\n",
            "  9166/100000: episode: 11, duration: 57.419s, episode steps: 623, steps per second:  11, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.404 [0.000, 5.000],  loss: 0.008331, mean_q: 0.127339, mean_eps: 0.100000\n",
            "  9847/100000: episode: 12, duration: 60.252s, episode steps: 681, steps per second:  11, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.269 [0.000, 5.000],  loss: 0.008482, mean_q: 0.138589, mean_eps: 0.100000\n",
            "Step 10000: saving model to checkpoints/dqn_v15_weights_10000.h5f\n",
            " 10545/100000: episode: 13, duration: 64.363s, episode steps: 698, steps per second:  11, episode reward: 21.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.009231, mean_q: 0.157864, mean_eps: 0.100000\n",
            "Step 11000: saving model to checkpoints/dqn_v15_weights_11000.h5f\n",
            " 11218/100000: episode: 14, duration: 60.228s, episode steps: 673, steps per second:  11, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010144, mean_q: 0.176659, mean_eps: 0.100000\n",
            " 11895/100000: episode: 15, duration: 59.601s, episode steps: 677, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.010274, mean_q: 0.198060, mean_eps: 0.100000\n",
            "Step 12000: saving model to checkpoints/dqn_v15_weights_12000.h5f\n",
            "Step 13000: saving model to checkpoints/dqn_v15_weights_13000.h5f\n",
            " 13057/100000: episode: 16, duration: 106.149s, episode steps: 1162, steps per second:  11, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.011373, mean_q: 0.239007, mean_eps: 0.100000\n",
            " 13928/100000: episode: 17, duration: 78.826s, episode steps: 871, steps per second:  11, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.010334, mean_q: 0.260058, mean_eps: 0.100000\n",
            "Step 14000: saving model to checkpoints/dqn_v15_weights_14000.h5f\n",
            " 14608/100000: episode: 18, duration: 61.188s, episode steps: 680, steps per second:  11, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.009718, mean_q: 0.270067, mean_eps: 0.100000\n",
            "Step 15000: saving model to checkpoints/dqn_v15_weights_15000.h5f\n",
            " 15103/100000: episode: 19, duration: 44.337s, episode steps: 495, steps per second:  11, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.939 [0.000, 5.000],  loss: 0.008528, mean_q: 0.259180, mean_eps: 0.100000\n",
            "Step 16000: saving model to checkpoints/dqn_v15_weights_16000.h5f\n",
            " 16150/100000: episode: 20, duration: 95.324s, episode steps: 1047, steps per second:  11, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.031 [0.000, 5.000],  loss: 0.008712, mean_q: 0.268169, mean_eps: 0.100000\n",
            "Step 17000: saving model to checkpoints/dqn_v15_weights_17000.h5f\n",
            " 17137/100000: episode: 21, duration: 89.896s, episode steps: 987, steps per second:  11, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.009851, mean_q: 0.266392, mean_eps: 0.100000\n",
            "Step 18000: saving model to checkpoints/dqn_v15_weights_18000.h5f\n",
            " 18105/100000: episode: 22, duration: 86.313s, episode steps: 968, steps per second:  11, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.008780, mean_q: 0.289490, mean_eps: 0.100000\n",
            " 18837/100000: episode: 23, duration: 65.501s, episode steps: 732, steps per second:  11, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.010608, mean_q: 0.317205, mean_eps: 0.100000\n",
            "Step 19000: saving model to checkpoints/dqn_v15_weights_19000.h5f\n",
            " 19777/100000: episode: 24, duration: 87.298s, episode steps: 940, steps per second:  11, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.991 [0.000, 5.000],  loss: 0.009892, mean_q: 0.292859, mean_eps: 0.100000\n",
            "Step 20000: saving model to checkpoints/dqn_v15_weights_20000.h5f\n",
            " 20611/100000: episode: 25, duration: 75.882s, episode steps: 834, steps per second:  11, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.783 [0.000, 5.000],  loss: 0.009827, mean_q: 0.303749, mean_eps: 0.100000\n",
            "Step 21000: saving model to checkpoints/dqn_v15_weights_21000.h5f\n",
            " 21257/100000: episode: 26, duration: 60.092s, episode steps: 646, steps per second:  11, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.720 [0.000, 5.000],  loss: 0.007942, mean_q: 0.310000, mean_eps: 0.100000\n",
            " 21661/100000: episode: 27, duration: 36.121s, episode steps: 404, steps per second:  11, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 3.163 [0.000, 5.000],  loss: 0.008800, mean_q: 0.335332, mean_eps: 0.100000\n",
            "Step 22000: saving model to checkpoints/dqn_v15_weights_22000.h5f\n",
            " 22797/100000: episode: 28, duration: 102.337s, episode steps: 1136, steps per second:  11, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.006829, mean_q: 0.320060, mean_eps: 0.100000\n",
            "Step 23000: saving model to checkpoints/dqn_v15_weights_23000.h5f\n",
            " 23444/100000: episode: 29, duration: 57.958s, episode steps: 647, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.006326, mean_q: 0.339262, mean_eps: 0.100000\n",
            "Step 24000: saving model to checkpoints/dqn_v15_weights_24000.h5f\n",
            " 24342/100000: episode: 30, duration: 80.893s, episode steps: 898, steps per second:  11, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.007058, mean_q: 0.364926, mean_eps: 0.100000\n",
            "Step 25000: saving model to checkpoints/dqn_v15_weights_25000.h5f\n",
            " 25032/100000: episode: 31, duration: 61.729s, episode steps: 690, steps per second:  11, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.275 [0.000, 5.000],  loss: 0.007219, mean_q: 0.389216, mean_eps: 0.100000\n",
            "Step 26000: saving model to checkpoints/dqn_v15_weights_26000.h5f\n",
            " 26454/100000: episode: 32, duration: 127.991s, episode steps: 1422, steps per second:  11, episode reward: 29.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.081 [0.000, 5.000],  loss: 0.008289, mean_q: 0.412696, mean_eps: 0.100000\n",
            "Step 27000: saving model to checkpoints/dqn_v15_weights_27000.h5f\n",
            " 27474/100000: episode: 33, duration: 92.744s, episode steps: 1020, steps per second:  11, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.008006, mean_q: 0.421799, mean_eps: 0.100000\n",
            "Step 28000: saving model to checkpoints/dqn_v15_weights_28000.h5f\n",
            " 28011/100000: episode: 34, duration: 48.435s, episode steps: 537, steps per second:  11, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.594 [0.000, 5.000],  loss: 0.009892, mean_q: 0.429291, mean_eps: 0.100000\n",
            " 28560/100000: episode: 35, duration: 49.338s, episode steps: 549, steps per second:  11, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.497 [0.000, 5.000],  loss: 0.009425, mean_q: 0.428154, mean_eps: 0.100000\n",
            "Step 29000: saving model to checkpoints/dqn_v15_weights_29000.h5f\n",
            " 29285/100000: episode: 36, duration: 66.409s, episode steps: 725, steps per second:  11, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.011011, mean_q: 0.445175, mean_eps: 0.100000\n",
            "Step 30000: saving model to checkpoints/dqn_v15_weights_30000.h5f\n",
            " 30061/100000: episode: 37, duration: 70.009s, episode steps: 776, steps per second:  11, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.011257, mean_q: 0.473391, mean_eps: 0.100000\n",
            " 30440/100000: episode: 38, duration: 33.091s, episode steps: 379, steps per second:  11, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.794 [0.000, 5.000],  loss: 0.007796, mean_q: 0.500219, mean_eps: 0.100000\n",
            "Step 31000: saving model to checkpoints/dqn_v15_weights_31000.h5f\n",
            " 31077/100000: episode: 39, duration: 59.004s, episode steps: 637, steps per second:  11, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.349 [0.000, 5.000],  loss: 0.009970, mean_q: 0.518255, mean_eps: 0.100000\n",
            " 31795/100000: episode: 40, duration: 62.814s, episode steps: 718, steps per second:  11, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.213 [0.000, 5.000],  loss: 0.008745, mean_q: 0.558392, mean_eps: 0.100000\n",
            "Step 32000: saving model to checkpoints/dqn_v15_weights_32000.h5f\n",
            " 32676/100000: episode: 41, duration: 78.842s, episode steps: 881, steps per second:  11, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.007694, mean_q: 0.582121, mean_eps: 0.100000\n",
            "Step 33000: saving model to checkpoints/dqn_v15_weights_33000.h5f\n",
            " 33819/100000: episode: 42, duration: 103.618s, episode steps: 1143, steps per second:  11, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.508 [0.000, 5.000],  loss: 0.008878, mean_q: 0.591024, mean_eps: 0.100000\n",
            "Step 34000: saving model to checkpoints/dqn_v15_weights_34000.h5f\n",
            " 34456/100000: episode: 43, duration: 59.219s, episode steps: 637, steps per second:  11, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 3.443 [0.000, 5.000],  loss: 0.007232, mean_q: 0.607468, mean_eps: 0.100000\n",
            "Step 35000: saving model to checkpoints/dqn_v15_weights_35000.h5f\n",
            " 35095/100000: episode: 44, duration: 57.726s, episode steps: 639, steps per second:  11, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.008442, mean_q: 0.603806, mean_eps: 0.100000\n",
            " 35757/100000: episode: 45, duration: 59.100s, episode steps: 662, steps per second:  11, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.858 [0.000, 5.000],  loss: 0.006937, mean_q: 0.578441, mean_eps: 0.100000\n",
            "Step 36000: saving model to checkpoints/dqn_v15_weights_36000.h5f\n",
            " 36813/100000: episode: 46, duration: 94.858s, episode steps: 1056, steps per second:  11, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.007155, mean_q: 0.593591, mean_eps: 0.100000\n",
            "Step 37000: saving model to checkpoints/dqn_v15_weights_37000.h5f\n",
            " 37478/100000: episode: 47, duration: 59.239s, episode steps: 665, steps per second:  11, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.007090, mean_q: 0.607715, mean_eps: 0.100000\n",
            "Step 38000: saving model to checkpoints/dqn_v15_weights_38000.h5f\n",
            " 38563/100000: episode: 48, duration: 97.674s, episode steps: 1085, steps per second:  11, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.007210, mean_q: 0.624787, mean_eps: 0.100000\n",
            "Step 39000: saving model to checkpoints/dqn_v15_weights_39000.h5f\n",
            " 39992/100000: episode: 49, duration: 130.073s, episode steps: 1429, steps per second:  11, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.007039, mean_q: 0.633404, mean_eps: 0.100000\n",
            "Step 40000: saving model to checkpoints/dqn_v15_weights_40000.h5f\n",
            " 40776/100000: episode: 50, duration: 69.860s, episode steps: 784, steps per second:  11, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.008971, mean_q: 0.647517, mean_eps: 0.100000\n",
            "Step 41000: saving model to checkpoints/dqn_v15_weights_41000.h5f\n",
            " 41684/100000: episode: 51, duration: 81.462s, episode steps: 908, steps per second:  11, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.008523, mean_q: 0.651089, mean_eps: 0.100000\n",
            "Step 42000: saving model to checkpoints/dqn_v15_weights_42000.h5f\n",
            " 42342/100000: episode: 52, duration: 58.766s, episode steps: 658, steps per second:  11, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.008968, mean_q: 0.656686, mean_eps: 0.100000\n",
            "Step 43000: saving model to checkpoints/dqn_v15_weights_43000.h5f\n",
            " 43035/100000: episode: 53, duration: 61.864s, episode steps: 693, steps per second:  11, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.008731, mean_q: 0.665334, mean_eps: 0.100000\n",
            " 43450/100000: episode: 54, duration: 36.911s, episode steps: 415, steps per second:  11, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.009323, mean_q: 0.698894, mean_eps: 0.100000\n",
            " 43930/100000: episode: 55, duration: 43.036s, episode steps: 480, steps per second:  11, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.007116, mean_q: 0.705310, mean_eps: 0.100000\n",
            "Step 44000: saving model to checkpoints/dqn_v15_weights_44000.h5f\n",
            " 44881/100000: episode: 56, duration: 85.970s, episode steps: 951, steps per second:  11, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.764 [0.000, 5.000],  loss: 0.007461, mean_q: 0.723505, mean_eps: 0.100000\n",
            "Step 45000: saving model to checkpoints/dqn_v15_weights_45000.h5f\n",
            " 45530/100000: episode: 57, duration: 58.925s, episode steps: 649, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.008 [0.000, 5.000],  loss: 0.008970, mean_q: 0.734806, mean_eps: 0.100000\n",
            "Step 46000: saving model to checkpoints/dqn_v15_weights_46000.h5f\n",
            " 46451/100000: episode: 58, duration: 82.595s, episode steps: 921, steps per second:  11, episode reward: 27.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.069 [0.000, 5.000],  loss: 0.009290, mean_q: 0.711668, mean_eps: 0.100000\n",
            " 46970/100000: episode: 59, duration: 45.964s, episode steps: 519, steps per second:  11, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.509 [0.000, 5.000],  loss: 0.008210, mean_q: 0.687685, mean_eps: 0.100000\n",
            "Step 47000: saving model to checkpoints/dqn_v15_weights_47000.h5f\n",
            " 47930/100000: episode: 60, duration: 84.968s, episode steps: 960, steps per second:  11, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.597 [0.000, 5.000],  loss: 0.009718, mean_q: 0.748288, mean_eps: 0.100000\n",
            "Step 48000: saving model to checkpoints/dqn_v15_weights_48000.h5f\n",
            " 48616/100000: episode: 61, duration: 62.696s, episode steps: 686, steps per second:  11, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.201 [0.000, 5.000],  loss: 0.010543, mean_q: 0.776039, mean_eps: 0.100000\n",
            "Step 49000: saving model to checkpoints/dqn_v15_weights_49000.h5f\n",
            " 49694/100000: episode: 62, duration: 96.534s, episode steps: 1078, steps per second:  11, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.231 [0.000, 5.000],  loss: 0.011310, mean_q: 0.775288, mean_eps: 0.100000\n",
            "Step 50000: saving model to checkpoints/dqn_v15_weights_50000.h5f\n",
            " 50877/100000: episode: 63, duration: 105.922s, episode steps: 1183, steps per second:  11, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.683 [0.000, 5.000],  loss: 0.010903, mean_q: 0.826385, mean_eps: 0.100000\n",
            "Step 51000: saving model to checkpoints/dqn_v15_weights_51000.h5f\n",
            " 51769/100000: episode: 64, duration: 80.956s, episode steps: 892, steps per second:  11, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.967 [0.000, 5.000],  loss: 0.009978, mean_q: 0.840973, mean_eps: 0.100000\n",
            "Step 52000: saving model to checkpoints/dqn_v15_weights_52000.h5f\n",
            " 52463/100000: episode: 65, duration: 63.308s, episode steps: 694, steps per second:  11, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.011421, mean_q: 0.843717, mean_eps: 0.100000\n",
            "Step 53000: saving model to checkpoints/dqn_v15_weights_53000.h5f\n",
            " 53275/100000: episode: 66, duration: 73.350s, episode steps: 812, steps per second:  11, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.999 [0.000, 5.000],  loss: 0.011997, mean_q: 0.859843, mean_eps: 0.100000\n",
            "Step 54000: saving model to checkpoints/dqn_v15_weights_54000.h5f\n",
            " 54104/100000: episode: 67, duration: 78.672s, episode steps: 829, steps per second:  11, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.963 [0.000, 5.000],  loss: 0.010717, mean_q: 0.872911, mean_eps: 0.100000\n",
            "Step 55000: saving model to checkpoints/dqn_v15_weights_55000.h5f\n",
            " 55228/100000: episode: 68, duration: 111.667s, episode steps: 1124, steps per second:  10, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.011463, mean_q: 0.899089, mean_eps: 0.100000\n",
            " 55884/100000: episode: 69, duration: 63.510s, episode steps: 656, steps per second:  10, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.011030, mean_q: 0.915331, mean_eps: 0.100000\n",
            "Step 56000: saving model to checkpoints/dqn_v15_weights_56000.h5f\n",
            " 56384/100000: episode: 70, duration: 47.694s, episode steps: 500, steps per second:  10, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.011962, mean_q: 0.919997, mean_eps: 0.100000\n",
            "Step 57000: saving model to checkpoints/dqn_v15_weights_57000.h5f\n",
            " 57186/100000: episode: 71, duration: 72.292s, episode steps: 802, steps per second:  11, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.011852, mean_q: 0.928358, mean_eps: 0.100000\n",
            "Step 58000: saving model to checkpoints/dqn_v15_weights_58000.h5f\n",
            " 58253/100000: episode: 72, duration: 97.138s, episode steps: 1067, steps per second:  11, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.012240, mean_q: 0.945526, mean_eps: 0.100000\n",
            " 58905/100000: episode: 73, duration: 58.054s, episode steps: 652, steps per second:  11, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.011185, mean_q: 0.947935, mean_eps: 0.100000\n",
            "Step 59000: saving model to checkpoints/dqn_v15_weights_59000.h5f\n",
            "Step 60000: saving model to checkpoints/dqn_v15_weights_60000.h5f\n",
            " 60030/100000: episode: 74, duration: 101.180s, episode steps: 1125, steps per second:  11, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.012114, mean_q: 0.945846, mean_eps: 0.100000\n",
            " 60561/100000: episode: 75, duration: 48.928s, episode steps: 531, steps per second:  11, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.010540, mean_q: 0.932317, mean_eps: 0.100000\n",
            "Step 61000: saving model to checkpoints/dqn_v15_weights_61000.h5f\n",
            " 61695/100000: episode: 76, duration: 102.145s, episode steps: 1134, steps per second:  11, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.806 [0.000, 5.000],  loss: 0.010215, mean_q: 0.963000, mean_eps: 0.100000\n",
            "Step 62000: saving model to checkpoints/dqn_v15_weights_62000.h5f\n",
            "Step 63000: saving model to checkpoints/dqn_v15_weights_63000.h5f\n",
            " 63286/100000: episode: 77, duration: 143.877s, episode steps: 1591, steps per second:  11, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.470 [0.000, 5.000],  loss: 0.010121, mean_q: 0.986773, mean_eps: 0.100000\n",
            "Step 64000: saving model to checkpoints/dqn_v15_weights_64000.h5f\n",
            " 64381/100000: episode: 78, duration: 98.880s, episode steps: 1095, steps per second:  11, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.928 [0.000, 5.000],  loss: 0.011039, mean_q: 0.993250, mean_eps: 0.100000\n",
            "Step 65000: saving model to checkpoints/dqn_v15_weights_65000.h5f\n",
            " 65310/100000: episode: 79, duration: 83.643s, episode steps: 929, steps per second:  11, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.034 [0.000, 5.000],  loss: 0.009471, mean_q: 1.009105, mean_eps: 0.100000\n",
            " 65938/100000: episode: 80, duration: 56.178s, episode steps: 628, steps per second:  11, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.009881, mean_q: 1.030156, mean_eps: 0.100000\n",
            "Step 66000: saving model to checkpoints/dqn_v15_weights_66000.h5f\n",
            "Step 67000: saving model to checkpoints/dqn_v15_weights_67000.h5f\n",
            " 67302/100000: episode: 81, duration: 122.622s, episode steps: 1364, steps per second:  11, episode reward: 28.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009541, mean_q: 1.045250, mean_eps: 0.100000\n",
            "Step 68000: saving model to checkpoints/dqn_v15_weights_68000.h5f\n",
            " 68492/100000: episode: 82, duration: 106.631s, episode steps: 1190, steps per second:  11, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.517 [0.000, 5.000],  loss: 0.010469, mean_q: 1.115654, mean_eps: 0.100000\n",
            "Step 69000: saving model to checkpoints/dqn_v15_weights_69000.h5f\n",
            " 69682/100000: episode: 83, duration: 105.231s, episode steps: 1190, steps per second:  11, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.650 [0.000, 5.000],  loss: 0.010507, mean_q: 1.235266, mean_eps: 0.100000\n",
            "Step 70000: saving model to checkpoints/dqn_v15_weights_70000.h5f\n",
            " 70322/100000: episode: 84, duration: 56.829s, episode steps: 640, steps per second:  11, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.009456, mean_q: 1.231735, mean_eps: 0.100000\n",
            " 70823/100000: episode: 85, duration: 44.846s, episode steps: 501, steps per second:  11, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.988 [0.000, 5.000],  loss: 0.011275, mean_q: 1.236558, mean_eps: 0.100000\n",
            "Step 71000: saving model to checkpoints/dqn_v15_weights_71000.h5f\n",
            " 71785/100000: episode: 86, duration: 85.139s, episode steps: 962, steps per second:  11, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.065 [0.000, 5.000],  loss: 0.012347, mean_q: 1.266444, mean_eps: 0.100000\n",
            "Step 72000: saving model to checkpoints/dqn_v15_weights_72000.h5f\n",
            " 72420/100000: episode: 87, duration: 55.508s, episode steps: 635, steps per second:  11, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.011154, mean_q: 1.303376, mean_eps: 0.100000\n",
            "Step 73000: saving model to checkpoints/dqn_v15_weights_73000.h5f\n",
            " 73533/100000: episode: 88, duration: 96.904s, episode steps: 1113, steps per second:  11, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.010770, mean_q: 1.356929, mean_eps: 0.100000\n",
            "Step 74000: saving model to checkpoints/dqn_v15_weights_74000.h5f\n",
            "Step 75000: saving model to checkpoints/dqn_v15_weights_75000.h5f\n",
            " 75505/100000: episode: 89, duration: 171.302s, episode steps: 1972, steps per second:  12, episode reward: 35.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.461 [0.000, 5.000],  loss: 0.011343, mean_q: 1.386220, mean_eps: 0.100000\n",
            "Step 76000: saving model to checkpoints/dqn_v15_weights_76000.h5f\n",
            " 76400/100000: episode: 90, duration: 79.379s, episode steps: 895, steps per second:  11, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.009821, mean_q: 1.294621, mean_eps: 0.100000\n",
            "Step 77000: saving model to checkpoints/dqn_v15_weights_77000.h5f\n",
            " 77056/100000: episode: 91, duration: 57.130s, episode steps: 656, steps per second:  11, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.880 [0.000, 5.000],  loss: 0.008278, mean_q: 1.263978, mean_eps: 0.100000\n",
            " 77944/100000: episode: 92, duration: 79.302s, episode steps: 888, steps per second:  11, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.010065, mean_q: 1.254244, mean_eps: 0.100000\n",
            "Step 78000: saving model to checkpoints/dqn_v15_weights_78000.h5f\n",
            " 78645/100000: episode: 93, duration: 61.409s, episode steps: 701, steps per second:  11, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.010233, mean_q: 1.236374, mean_eps: 0.100000\n",
            "Step 79000: saving model to checkpoints/dqn_v15_weights_79000.h5f\n",
            " 79280/100000: episode: 94, duration: 56.196s, episode steps: 635, steps per second:  11, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.956 [0.000, 5.000],  loss: 0.009109, mean_q: 1.217584, mean_eps: 0.100000\n",
            "Step 80000: saving model to checkpoints/dqn_v15_weights_80000.h5f\n",
            " 80176/100000: episode: 95, duration: 78.301s, episode steps: 896, steps per second:  11, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.008401, mean_q: 1.263362, mean_eps: 0.100000\n",
            " 80916/100000: episode: 96, duration: 66.387s, episode steps: 740, steps per second:  11, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.009766, mean_q: 1.331138, mean_eps: 0.100000\n",
            "Step 81000: saving model to checkpoints/dqn_v15_weights_81000.h5f\n",
            " 81424/100000: episode: 97, duration: 45.911s, episode steps: 508, steps per second:  11, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 0.010926, mean_q: 1.370002, mean_eps: 0.100000\n",
            "Step 82000: saving model to checkpoints/dqn_v15_weights_82000.h5f\n",
            " 82356/100000: episode: 98, duration: 84.032s, episode steps: 932, steps per second:  11, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.010581, mean_q: 1.371746, mean_eps: 0.100000\n",
            "Step 83000: saving model to checkpoints/dqn_v15_weights_83000.h5f\n",
            " 83322/100000: episode: 99, duration: 86.536s, episode steps: 966, steps per second:  11, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.688 [0.000, 5.000],  loss: 0.009246, mean_q: 1.411959, mean_eps: 0.100000\n",
            "Step 84000: saving model to checkpoints/dqn_v15_weights_84000.h5f\n",
            " 84513/100000: episode: 100, duration: 104.448s, episode steps: 1191, steps per second:  11, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.010900, mean_q: 1.416827, mean_eps: 0.100000\n",
            "Step 85000: saving model to checkpoints/dqn_v15_weights_85000.h5f\n",
            " 85663/100000: episode: 101, duration: 102.389s, episode steps: 1150, steps per second:  11, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.010117, mean_q: 1.409888, mean_eps: 0.100000\n",
            "Step 86000: saving model to checkpoints/dqn_v15_weights_86000.h5f\n",
            " 86115/100000: episode: 102, duration: 40.567s, episode steps: 452, steps per second:  11, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.011125, mean_q: 1.408203, mean_eps: 0.100000\n",
            " 86740/100000: episode: 103, duration: 56.647s, episode steps: 625, steps per second:  11, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.009774, mean_q: 1.419353, mean_eps: 0.100000\n",
            "Step 87000: saving model to checkpoints/dqn_v15_weights_87000.h5f\n",
            " 87646/100000: episode: 104, duration: 81.985s, episode steps: 906, steps per second:  11, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.837 [0.000, 5.000],  loss: 0.010348, mean_q: 1.447221, mean_eps: 0.100000\n",
            "Step 88000: saving model to checkpoints/dqn_v15_weights_88000.h5f\n",
            " 88219/100000: episode: 105, duration: 52.313s, episode steps: 573, steps per second:  11, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.010955, mean_q: 1.446162, mean_eps: 0.100000\n",
            "Step 89000: saving model to checkpoints/dqn_v15_weights_89000.h5f\n",
            " 89652/100000: episode: 106, duration: 129.015s, episode steps: 1433, steps per second:  11, episode reward: 27.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.011628, mean_q: 1.480712, mean_eps: 0.100000\n",
            "Step 90000: saving model to checkpoints/dqn_v15_weights_90000.h5f\n",
            " 90175/100000: episode: 107, duration: 46.971s, episode steps: 523, steps per second:  11, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.061 [0.000, 5.000],  loss: 0.011933, mean_q: 1.466018, mean_eps: 0.100000\n",
            " 90517/100000: episode: 108, duration: 32.066s, episode steps: 342, steps per second:  11, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.977 [0.000, 5.000],  loss: 0.010829, mean_q: 1.446279, mean_eps: 0.100000\n",
            "Step 91000: saving model to checkpoints/dqn_v15_weights_91000.h5f\n",
            " 91558/100000: episode: 109, duration: 94.281s, episode steps: 1041, steps per second:  11, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.260 [0.000, 5.000],  loss: 0.011827, mean_q: 1.487725, mean_eps: 0.100000\n",
            "Step 92000: saving model to checkpoints/dqn_v15_weights_92000.h5f\n",
            " 92741/100000: episode: 110, duration: 105.934s, episode steps: 1183, steps per second:  11, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.995 [0.000, 5.000],  loss: 0.009252, mean_q: 1.419166, mean_eps: 0.100000\n",
            "Step 93000: saving model to checkpoints/dqn_v15_weights_93000.h5f\n",
            " 93911/100000: episode: 111, duration: 105.324s, episode steps: 1170, steps per second:  11, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.010876, mean_q: 1.403473, mean_eps: 0.100000\n",
            "Step 94000: saving model to checkpoints/dqn_v15_weights_94000.h5f\n",
            "Step 95000: saving model to checkpoints/dqn_v15_weights_95000.h5f\n",
            " 95490/100000: episode: 112, duration: 141.739s, episode steps: 1579, steps per second:  11, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.010496, mean_q: 1.436670, mean_eps: 0.100000\n",
            " 95865/100000: episode: 113, duration: 33.886s, episode steps: 375, steps per second:  11, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.189 [0.000, 5.000],  loss: 0.011768, mean_q: 1.418921, mean_eps: 0.100000\n",
            "Step 96000: saving model to checkpoints/dqn_v15_weights_96000.h5f\n",
            " 96353/100000: episode: 114, duration: 44.473s, episode steps: 488, steps per second:  11, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.011716, mean_q: 1.454256, mean_eps: 0.100000\n",
            " 96685/100000: episode: 115, duration: 29.381s, episode steps: 332, steps per second:  11, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.011270, mean_q: 1.451557, mean_eps: 0.100000\n",
            "Step 97000: saving model to checkpoints/dqn_v15_weights_97000.h5f\n",
            " 97045/100000: episode: 116, duration: 31.939s, episode steps: 360, steps per second:  11, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.011161, mean_q: 1.491212, mean_eps: 0.100000\n",
            " 97433/100000: episode: 117, duration: 35.654s, episode steps: 388, steps per second:  11, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.009779, mean_q: 1.512529, mean_eps: 0.100000\n",
            "Step 98000: saving model to checkpoints/dqn_v15_weights_98000.h5f\n",
            " 98679/100000: episode: 118, duration: 110.878s, episode steps: 1246, steps per second:  11, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.009536, mean_q: 1.538995, mean_eps: 0.100000\n",
            "Step 99000: saving model to checkpoints/dqn_v15_weights_99000.h5f\n",
            " 99464/100000: episode: 119, duration: 69.892s, episode steps: 785, steps per second:  11, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.010192, mean_q: 1.558752, mean_eps: 0.100000\n",
            " 99851/100000: episode: 120, duration: 33.914s, episode steps: 387, steps per second:  11, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.010812, mean_q: 1.576739, mean_eps: 0.100000\n",
            "Step 100000: saving model to checkpoints/dqn_v15_weights_100000.h5f\n",
            "done, took 8929.867 seconds\n",
            "⏱ Tiempo de fin: 16:31:20\n",
            "[WARNING] dqn_v15_weights.h5f.index already exists - overwrite? [y/n]y\n",
            "[TIP] Next time specify overwrite=True!\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"⏱ Tiempo de inicio: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "dqn.fit(env, nb_steps=100000, visualize=False, verbose=2,callbacks=[checkpoint_callback])\n",
        "\n",
        "print(f\"⏱ Tiempo de fin: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "# Guardando modelo\n",
        "dqn.save_weights('dqn_v15_weights.h5f')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSVcMgOsR0Nv",
        "outputId": "3ae98b9b-610f-4853-fe5c-16b0a7ef8aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 18.000, steps: 781\n",
            "Episode 2: reward: 12.000, steps: 638\n",
            "Episode 3: reward: 15.000, steps: 684\n",
            "Episode 4: reward: 14.000, steps: 794\n",
            "Episode 5: reward: 6.000, steps: 367\n",
            "Episode 6: reward: 10.000, steps: 510\n",
            "Episode 7: reward: 13.000, steps: 666\n",
            "Episode 8: reward: 3.000, steps: 347\n",
            "Episode 9: reward: 7.000, steps: 390\n",
            "Episode 10: reward: 19.000, steps: 1436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7884d04e5e10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward a los 500k\n",
        "#weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "weights_filename = 'dqn_v15_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ltUB-jMIXQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "miar_rl",
      "language": "python",
      "name": "miar_rl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}